{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current sessionv","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-13T14:52:56.767640Z","iopub.execute_input":"2025-09-13T14:52:56.767835Z","iopub.status.idle":"2025-09-13T14:52:58.821564Z","shell.execute_reply.started":"2025-09-13T14:52:56.767816Z","shell.execute_reply":"2025-09-13T14:52:58.820736Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"!pip install pymysql","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-14T21:04:11.001929Z","iopub.execute_input":"2025-09-14T21:04:11.002173Z","iopub.status.idle":"2025-09-14T21:04:15.884286Z","shell.execute_reply.started":"2025-09-14T21:04:11.002155Z","shell.execute_reply":"2025-09-14T21:04:15.883545Z"}},"outputs":[{"name":"stdout","text":"Collecting pymysql\n  Downloading pymysql-1.1.2-py3-none-any.whl.metadata (4.3 kB)\nDownloading pymysql-1.1.2-py3-none-any.whl (45 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.3/45.3 kB\u001b[0m \u001b[31m87.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: pymysql\nSuccessfully installed pymysql-1.1.2\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# Provenance Radar++ — AIC(3k) + Cleveland ingest, batched embeddings, robust DB retries (GPU optional)\n\n# -------- quiet & device selection --------\nimport os\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\nos.environ[\"TRANSFORMERS_VERBOSITY\"] = \"error\"\nos.environ[\"HF_HUB_DISABLE_TELEMETRY\"] = \"1\"\n\nUSE_GPU = os.getenv(\"USE_GPU\", \"1\") == \"1\"  # set to \"1\" if you turned on Kaggle GPU\nif not USE_GPU:\n    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"   # force CPU unless USE_GPU=1\n\n# -------- std imports --------\nimport re, json, time, warnings\nimport certifi, pymysql, requests, torch\nfrom datetime import datetime\nfrom sentence_transformers import SentenceTransformer\nfrom transformers.utils.logging import set_verbosity_error\nfrom pymysql.err import OperationalError\n\n# Progress bar import (FIXED to be notebook-aware)\ndef _is_notebook() -> bool:\n    \"\"\"Checks if the code is running in a Jupyter-like notebook environment.\"\"\"\n    try:\n        from IPython import get_ipython\n        shell = get_ipython().__class__.__name__\n        if 'Shell' in shell:\n            return shell == 'ZMQInteractiveShell' # Jupyter, Kaggle, Colab\n        return False\n    except (NameError, ImportError):\n        return False # Not in an IPython environment\n\ntry:\n    if _is_notebook():\n        from tqdm.notebook import tqdm\n    else:\n        from tqdm import tqdm\n    HAS_TQDM = True\nexcept ImportError:\n    HAS_TQDM = False\n    print(\"Install tqdm for better progress bars: pip install tqdm\")\n\n\nwarnings.filterwarnings(\"ignore\")\nset_verbosity_error()\n\n# =============================\n# Config (tweak here)\n# =============================\nDB_NAME        = os.getenv(\"TIDB_DB\", \"test\")\nTARGET_VEC_DIM = int(os.getenv(\"VEC_DIM\", \"1536\"))\n\n# AIC: page through to target (fast + reliable)\nAIC_TARGET     = int(os.getenv(\"AIC_TARGET\", \"3000\"))\nAIC_PAGE_SIZE  = int(os.getenv(\"AIC_PAGE_SIZE\", \"100\"))\n\n# Cleveland Museum of Art (no key)\nCMA_TARGET = int(os.getenv(\"CMA_TARGET\", \"0\"))  # was 1500\nCMA_PAGE_SIZE      = int(os.getenv(\"CMA_PAGE_SIZE\", \"100\"))\nCMA_MAX_TIME_SEC = int(os.getenv(\"CMA_MAX_TIME_SEC\", \"600\"))   # bail after time budget\n\n# batching controls\nSENT_FLUSH_BATCH   = int(os.getenv(\"SENT_FLUSH_BATCH\", \"1200\"))  # sentences per flush to DB\nEMBED_BATCH_SIZE   = int(os.getenv(\"EMBED_BATCH_SIZE\", \"128\"))   # model batch size\nBULK_COMMIT_EVERY  = int(os.getenv(\"BULK_COMMIT_EVERY\", \"400\"))  # commit every N objects\n\n# TiDB conn (your defaults)\nTIDB_HOST = os.getenv(\"TIDB_HOST\", \"gateway01.ap-northeast-1.prod.aws.tidbcloud.com\")\nTIDB_PORT = int(os.getenv(\"TIDB_PORT\", \"4000\"))\nTIDB_USER = os.getenv(\"TIDB_USER\", \"user\")\nTIDB_PASS = os.getenv(\"TIDB_PASS\", \"password\")\n\n# =============================\n# Global connection + helpers\n# =============================\nconn = None\ncursor = None\n\ndef _connect():\n    global conn, cursor\n    try:\n        if conn: conn.close()\n    except Exception:\n        pass\n    conn = pymysql.connect(\n        host=TIDB_HOST, port=TIDB_PORT, user=TIDB_USER, password=TIDB_PASS,\n        database=DB_NAME, ssl={\"ca\": certifi.where()},\n        ssl_verify_cert=True, ssl_verify_identity=True, autocommit=True,\n    )\n    cursor = conn.cursor()\n    cursor.execute(f\"CREATE DATABASE IF NOT EXISTS `{DB_NAME}`;\")\n    cursor.execute(f\"USE `{DB_NAME}`;\")\n\ndef _is_recoverable(op_err: OperationalError) -> bool:\n    return isinstance(op_err, OperationalError) and op_err.args and op_err.args[0] in (2006, 2013)\n\ndef run(sql, args=None, many=False, max_retries=3):\n    \"\"\"Execute SQL with auto-reconnect + retry.\"\"\"\n    global conn, cursor\n    for attempt in range(1, max_retries + 1):\n        try:\n            conn.ping(reconnect=True)\n            if many:\n                cursor.executemany(sql, args or [])\n            else:\n                cursor.execute(sql, args or ())\n            return\n        except OperationalError as e:\n            if _is_recoverable(e) and attempt < max_retries:\n                time.sleep(0.5 * attempt)\n                _connect()\n                continue\n            raise\n\ndef run_fetchall(sql, args=None, max_retries=3):\n    run(sql, args=args, max_retries=max_retries)\n    return cursor.fetchall()\n\ndef run_fetchone(sql, args=None, max_retries=3):\n    run(sql, args=args, max_retries=max_retries)\n    return cursor.fetchone()\n\n_connect()\n\n# =============================\n# Embeddings (batched) + device\n# =============================\ndevice = \"cuda\" if (USE_GPU and torch.cuda.is_available()) else \"cpu\"\nmodel = SentenceTransformer(\"all-MiniLM-L6-v2\", device=device)  # 384-dim\n\ndef _pad(vec, dim=TARGET_VEC_DIM):\n    return vec[:dim] + [0.0] * max(0, dim - len(vec))\n\ndef embed_batch(sentences):\n    if not sentences: return []\n    embs = model.encode(sentences, batch_size=EMBED_BATCH_SIZE, show_progress_bar=False, convert_to_numpy=True)\n    return [_pad(e.tolist()) for e in embs]\n\n# =============================\n# Heuristics: events / risks\n# =============================\nEVENT_PATTERNS = {\n    \"SOLD\": r\"\\bsold\\b|\\bauction\\b|\\bpurchased\\b|\\bbought\\b\",\n    \"SEIZED\": r\"\\b(seized|confiscated|looted|spoliated)\\b\",\n    \"ACQUIRED\": r\"\\bacquired\\b|\\bobtained\\b|\\bcollection of\\b\",\n    \"EXPORTED\": r\"\\bexported\\b|\\bremoved from\\b|\\bsmuggled\\b\",\n    \"LOOTED\": r\"\\blooted\\b|\\bspoliated\\b\",\n    \"STOLEN\": r\"\\bstolen\\b|\\btheft\\b|\\bpilfered\\b\",\n}\nRISK_KEYWORDS = [\"seized\",\"confiscated\",\"looted\",\"spoliated\",\"missing\",\"stolen\",\"illicit\",\"smuggled\",\"exported\"]\nNAZI_START, NAZI_END = 1933, 1945\n\ndef extract_years(sentence: str):\n    years = re.findall(r\"\\b(1[5-9]\\d{2}|20\\d{2})\\b\", sentence)\n    if not years: return None, None\n    y0 = int(years[0]); y1 = int(years[-1]) if len(years) > 1 else None\n    from_d = datetime(y0,1,1).date()\n    to_d   = datetime(y1,1,1).date() if y1 else None\n    return from_d, to_d\n\ndef extract_events(sentence: str):\n    out = []\n    for etype, pattern in EVENT_PATTERNS.items():\n        if re.search(pattern, sentence, re.IGNORECASE):\n            df, dt = extract_years(sentence)\n            out.append((etype, df, dt))\n    return out\n\ndef detect_textual_risks(sentence: str):\n    risks = []\n    s = sentence.lower()\n    for kw in RISK_KEYWORDS:\n        if kw in s:\n            base = 0.9 if kw in {\"looted\",\"stolen\",\"confiscated\"} else 0.6\n            risks.append((f\"KEYWORD_{kw.upper()}\", f\"Matched keyword '{kw}'\", base))\n    if \"private collection\" in s and \"of \" not in s:\n        risks.append((\"PRIVATE_COLLECTION_UNKNOWN\",\"'private collection' without owner\",0.3))\n    if any(w in s for w in [\"sold\",\"auction\",\"dealer\"]) and re.search(r\"\\b(193[3-9]|194[0-5])\\b\", s):\n        risks.append((\"NAZI_ERA_TRADE_WINDOW\",\"Commercial activity in 1933–1945\",0.6))\n    return risks\n\ndef detect_event_risks(events):\n    risks = []\n    for etype, df, _ in events:\n        year = df.year if df else None\n        if etype in {\"SEIZED\",\"LOOTED\",\"STOLEN\"}:\n            risks.append((f\"EVENT_{etype}\", f\"Event type {etype}\", 1.0))\n        elif etype == \"EXPORTED\":\n            risks.append((\"EVENT_EXPORTED\", f\"Exported (year={year})\", 0.8 if (year and year >= 1970) else 0.5))\n        elif etype == \"SOLD\":\n            if year and NAZI_START <= year <= NAZI_END:\n                risks.append((\"EVENT_SOLD_NAZI_WINDOW\",\"Sold 1933–1945\",0.6))\n            else:\n                risks.append((\"EVENT_SOLD\",\"Sold (neutral)\",0.2))\n    return risks\n\n# =============================\n# Schema bootstrap (idempotent)\n# =============================\ndef table_exists(schema, name):\n    row = run_fetchone(\n        \"SELECT 1 FROM information_schema.tables WHERE table_schema=%s AND table_name=%s LIMIT 1\",\n        (schema, name),\n    )\n    return row is not None\n\ndef bootstrap_schema():\n    run(f\"CREATE DATABASE IF NOT EXISTS `{DB_NAME}`;\")\n    run(f\"USE `{DB_NAME}`;\")\n\n    if not table_exists(DB_NAME, \"objects\"):\n        run(\"\"\"\n        CREATE TABLE objects (\n          object_id BIGINT PRIMARY KEY AUTO_RANDOM,\n          source VARCHAR(32) NOT NULL,\n          source_object_id VARCHAR(128),\n          title TEXT, creator TEXT, object_type VARCHAR(64),\n          date_display VARCHAR(128), culture VARCHAR(128), country VARCHAR(128),\n          wd_qid VARCHAR(32), inventory_no VARCHAR(128), collection VARCHAR(256),\n          image_url TEXT,\n          risk_score DECIMAL(6,3) DEFAULT 0,\n          last_seen TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n        );\"\"\")\n    \n    # Add image_url column if it doesn't exist (safe for your existing DB)\n    try:\n        run(\"ALTER TABLE objects ADD COLUMN image_url TEXT;\")\n    except Exception:\n        pass  # Column already exists (your manual addition is fine!)\n\n    if not table_exists(DB_NAME, \"provenance_sentences\"):\n        run(f\"\"\"\n        CREATE TABLE provenance_sentences (\n          sent_id BIGINT PRIMARY KEY AUTO_RANDOM,\n          object_id BIGINT NOT NULL,\n          seq INT, sentence TEXT,\n          embedding VECTOR({TARGET_VEC_DIM}),\n          CONSTRAINT fk_ps_object FOREIGN KEY (object_id) REFERENCES objects(object_id)\n        );\"\"\")\n\n    if not table_exists(DB_NAME, \"provenance_events\"):\n        run(\"\"\"\n        CREATE TABLE provenance_events (\n          event_id BIGINT PRIMARY KEY AUTO_RANDOM,\n          object_id BIGINT NOT NULL,\n          event_type ENUM('ACQUIRED','SOLD','LOOTED','SEIZED','EXCAVATED','EXPORTED','ON_LOAN','UNKNOWN') DEFAULT 'UNKNOWN',\n          date_from DATE, date_to DATE, place TEXT, actor TEXT, method TEXT, source_ref TEXT,\n          CONSTRAINT fk_pe_object FOREIGN KEY (object_id) REFERENCES objects(object_id)\n        );\"\"\")\n\n    if not table_exists(DB_NAME, \"risk_signals\"):\n        run(\"\"\"\n        CREATE TABLE risk_signals (\n          signal_id BIGINT PRIMARY KEY AUTO_RANDOM,\n          object_id BIGINT NOT NULL,\n          code VARCHAR(64), detail TEXT, weight DECIMAL(6,3),\n          CONSTRAINT fk_rs_object FOREIGN KEY (object_id) REFERENCES objects(object_id)\n        );\"\"\")\n\n    run(\"DROP VIEW IF EXISTS flagged_leads;\")\n    run(\"\"\"\n    CREATE VIEW flagged_leads AS\n    SELECT\n      o.object_id, o.source, o.title, o.creator, o.risk_score,\n      (\n        SELECT GROUP_CONCAT(code ORDER BY weight DESC SEPARATOR ', ')\n        FROM (\n          SELECT rs.code, rs.weight\n          FROM risk_signals AS rs\n          WHERE rs.object_id = o.object_id\n          ORDER BY rs.weight DESC\n          LIMIT 3\n        ) AS top3\n      ) AS top_signals\n    FROM objects AS o\n    WHERE o.risk_score > 0\n    ORDER BY o.risk_score DESC;\"\"\")\n\n    conn.commit()\n    print(\"✅ Schema ready.\")\n\n# =============================\n# Batched buffers for speed\n# =============================\nclass IngestBuffer:\n    def __init__(self, flush_batch=SENT_FLUSH_BATCH):\n        self.flush_batch = flush_batch\n        self.sent_rows = []   # (object_id, seq, sentence)\n        self.event_rows = []  # (object_id, etype, df, dt, place, actor, method, src)\n        self.risk_rows  = []  # (object_id, code, detail, weight)\n\n    def add_sentence(self, object_id, seq, sentence, src_label):\n        self.sent_rows.append((object_id, seq, sentence))\n        evs = extract_events(sentence)\n        for etype, df, dt in evs:\n            self.event_rows.append((object_id, etype, df, dt, None, None, None, src_label))\n        for code, detail, w in detect_textual_risks(sentence):\n            self.risk_rows.append((object_id, code, detail, float(w)))\n        for code, detail, w in detect_event_risks(evs):\n            self.risk_rows.append((object_id, code, detail, float(w)))\n\n        if len(self.sent_rows) >= self.flush_batch:\n            self.flush()\n\n    def flush(self):\n        if self.sent_rows:\n            sentences = [s for (_,_,s) in self.sent_rows]\n            embs = embed_batch(sentences)\n            args = []\n            for (obj_id, seq, snt), emb in zip(self.sent_rows, embs):\n                args.append((obj_id, seq, snt, json.dumps(emb)))\n            run(\"\"\"INSERT INTO provenance_sentences (object_id, seq, sentence, embedding)\n                   VALUES (%s,%s,%s,%s)\"\"\", args=args, many=True)\n            self.sent_rows.clear()\n\n        if self.event_rows:\n            run(\"\"\"INSERT INTO provenance_events\n                   (object_id, event_type, date_from, date_to, place, actor, method, source_ref)\n                   VALUES (%s,%s,%s,%s,%s,%s,%s,%s)\"\"\", args=self.event_rows, many=True)\n            self.event_rows.clear()\n\n        if self.risk_rows:\n            run(\"\"\"INSERT INTO risk_signals (object_id, code, detail, weight)\n                   VALUES (%s,%s,%s,%s)\"\"\", args=self.risk_rows, many=True)\n            self.risk_rows.clear()\n\n        conn.commit()\n\nbuf = IngestBuffer()\n\n# =============================\n# Helpers\n# =============================\n\ndef get_or_create_object(source, sid, title, creator, date_display, country, image_url=None):\n    row = run_fetchone(\n        \"SELECT object_id FROM objects WHERE source=%s AND source_object_id=%s\",\n        (source, str(sid)),\n    )\n    if row:\n        # Update image_url if it's missing and we have one\n        if image_url:\n            run(\"UPDATE objects SET image_url=%s WHERE object_id=%s AND (image_url IS NULL OR image_url = '')\", \n                (image_url, row[0]))\n        return row[0]\n\n    # Insert new object with image_url\n    run(\"\"\"INSERT INTO objects (source, source_object_id, title, creator, date_display, country, image_url)\n           VALUES (%s,%s,%s,%s,%s,%s,%s)\"\"\",\n        (source, str(sid), title or \"\", creator or \"\", date_display or \"\", country or \"\", image_url))\n\n    row2 = run_fetchone(\n        \"SELECT object_id FROM objects WHERE source=%s AND source_object_id=%s\",\n        (source, str(sid)),\n    )\n    return row2[0]\n\ndef chunk_sentences(txt: str):\n    parts = re.split(r\"[.;]\\s+\", txt.replace(\"\\n\", \" \").strip())\n    return [p.strip() for p in parts if len(p.strip()) > 6]\n\n# =============================\n# AIC: paginated ingest to target (FIXED)\n# =============================\ndef ingest_aic_paged(target=AIC_TARGET, page_size=AIC_PAGE_SIZE, start_page=1):\n    s = requests.Session()\n    headers = {\"User-Agent\": \"ProvenanceRadar/0.4\"}\n    ingested = 0; page = start_page\n    \n    pbar = None\n    if HAS_TQDM:\n        pbar = tqdm(total=target, desc=\"AIC Ingest\", unit=\"obj\")\n    \n    while ingested < target:\n        # Request only image_id - we'll construct the IIIF URL ourselves\n        url = (\"https://api.artic.edu/api/v1/artworks\"\n               f\"?page={page}&limit={page_size}\"\n               \"&fields=id,title,artist_title,date_display,provenance_text,place_of_origin,image_id\")\n        \n        r = s.get(url, headers=headers, timeout=60)\n        r.raise_for_status()\n        \n        data = r.json().get(\"data\", [])\n        if not data: break\n\n        for obj in data:\n            prov = obj.get(\"provenance_text\")\n            if not prov: continue\n\n            # Construct the proper AIC IIIF image URL\n            image_url = None\n            image_id = obj.get(\"image_id\")\n            if image_id:\n                image_url = f\"https://www.artic.edu/iiif/2/{image_id}/full/843,/0/default.jpg\"\n\n            oid = get_or_create_object(\n                \"AIC\", \n                obj.get(\"id\"), \n                obj.get(\"title\"),\n                obj.get(\"artist_title\"), \n                obj.get(\"date_display\"),\n                obj.get(\"place_of_origin\", \"\"),\n                image_url\n            )\n            \n            for i, snt in enumerate(chunk_sentences(prov)):\n                buf.add_sentence(oid, i, snt, \"AIC\")\n            ingested += 1\n\n            if HAS_TQDM:\n                pbar.update(1)\n\n            if ingested % BULK_COMMIT_EVERY == 0:\n                buf.flush()\n                if not HAS_TQDM:\n                    print(f\"[AIC] committed {ingested}/{target} …\")\n            if ingested >= target: break\n        page += 1\n\n    if HAS_TQDM and pbar:\n        pbar.close()\n    \n    buf.flush()\n    print(f\"[AIC] total ingested with provenance: {ingested}\")\n    return ingested\n\n# =============================\n# CMA: resilient paginated ingest (no key)\n# Tries both known endpoints and data shapes; only keeps objects with provenance text.\n# =============================\nCMA_ENDPOINTS = [\n    \"https://openaccess-api.clevelandart.org/api/collection\",\n    \"https://openaccess-api.clevelandart.org/api/artworks\",\n]\n\ndef _extract_cma_data(payload):\n    # Common shapes: {\"data\":[...]} or {\"results\":[...]}\n    return payload.get(\"data\") or payload.get(\"results\") or []\n\ndef _extract_cma_provenance(obj):\n    # Try common keys\n    for k in (\"provenance\", \"provenance_text\", \"provenanceDescription\", \"provenance_display\"):\n        v = obj.get(k)\n        if isinstance(v, str) and v.strip():\n            return v.strip()\n    return \"\"\n\ndef _extract_cma_id(obj):\n    # Prefer id-like keys\n    for k in (\"id\", \"objectid\", \"object_id\", \"accession_number\"):\n        if obj.get(k) is not None:\n            return obj.get(k)\n    # fallback: try \"uuid\" or \"inventory\"\n    for k in (\"uuid\",\"inventory_number\"):\n        if obj.get(k) is not None:\n            return obj.get(k)\n    return None\n\ndef _extract_cma_title(obj):\n    for k in (\"title\",\"object_title\",\"primary_title\"): \n        if obj.get(k): return obj.get(k)\n    return obj.get(\"name\") or \"\"\n\ndef _extract_cma_creator(obj):\n    for k in (\"creator\",\"artist\",\"maker\",\"artist_display_name\"):\n        if obj.get(k): return obj.get(k)\n    return \"\"\n\ndef _extract_cma_date(obj):\n    for k in (\"creation_date\",\"dated\",\"date\",\"display_date\"):\n        if obj.get(k): return obj.get(k)\n    return \"\"\n\ndef _extract_cma_culture(obj):\n    for k in (\"culture\",\"culture_name\",\"nationality\",\"country\"):\n        if obj.get(k): return obj.get(k)\n    return \"\"\n\ndef ingest_cma(target=CMA_TARGET, page_size=CMA_PAGE_SIZE, max_time_sec=CMA_MAX_TIME_SEC):\n    sess = requests.Session()\n    headers = {\"User-Agent\": \"ProvenanceRadar/0.4\"}\n    ingested = 0\n    start_time = time.time()\n\n    pbar = None\n    if HAS_TQDM:\n        pbar = tqdm(total=target, desc=\"CMA Ingest\", unit=\"obj\")\n\n    # Iterate pages; stop when empty, time budget exceeded, or target met.\n    page = 0\n    while ingested < target and (time.time() - start_time) < max_time_sec:\n        got_page = False\n        for ep in CMA_ENDPOINTS:\n            try:\n                # Try \"limit/skip\"; many CMA examples support this.\n                params = {\"limit\": page_size, \"skip\": page * page_size}\n                r = sess.get(ep, params=params, headers=headers, timeout=60)\n                if r.status_code != 200:\n                    continue\n                items = _extract_cma_data(r.json())\n                if not isinstance(items, list) or not items:\n                    continue\n                got_page = True\n                for obj in items:\n                    prov = _extract_cma_provenance(obj)\n                    if not prov:\n                        continue\n                    oid = _extract_cma_id(obj)\n                    if oid is None:\n                        continue\n                    title   = _extract_cma_title(obj)\n                    creator = _extract_cma_creator(obj)\n                    dated   = _extract_cma_date(obj)\n                    culture = _extract_cma_culture(obj)\n                    dbid = get_or_create_object(\"CMA\", oid, title, creator, dated, culture)\n                    for i, snt in enumerate(chunk_sentences(prov)):\n                        buf.add_sentence(dbid, i, snt, \"CMA\")\n                    ingested += 1\n                    \n                    if HAS_TQDM:\n                        pbar.update(1)\n                    \n                    if ingested % BULK_COMMIT_EVERY == 0:\n                        buf.flush()\n                        if not HAS_TQDM:\n                            print(f\"[CMA] committed {ingested}/{target} …\")\n                    if ingested >= target or (time.time() - start_time) >= max_time_sec:\n                        break\n                if ingested >= target or (time.time() - start_time) >= max_time_sec:\n                    break\n            except Exception:\n                # Try next endpoint if this one fails\n                continue\n\n        if not got_page:\n            # No endpoint produced data for this page; stop.\n            break\n        page += 1\n\n    if HAS_TQDM and pbar:\n        pbar.close()\n    \n    buf.flush()\n    print(f\"[CMA] total ingested with provenance: {ingested} (time {int(time.time()-start_time)}s)\")\n    return ingested\n\n# =============================\n# Rescoring + reporting (WITH PROGRESS BAR!)\n# =============================\ndef rescore_all_objects():\n    # Get all object IDs\n    ids = [r[0] for r in run_fetchall(\"SELECT object_id FROM objects\")]\n    total_objects = len(ids)\n    \n    if total_objects == 0:\n        print(\"No objects to rescore.\")\n        return\n    \n    print(f\"Rescoring {total_objects:,} objects...\")\n    \n    # Process in batches for better performance and progress tracking\n    batch_size = 100\n    processed = 0\n    \n    pbar = None\n    if HAS_TQDM:\n        pbar = tqdm(total=total_objects, desc=\"Rescoring\", unit=\"obj\", \n                    bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}]')\n    \n    for i in range(0, total_objects, batch_size):\n        batch_ids = ids[i:i + batch_size]\n        \n        for oid in batch_ids:\n            total_risk = run_fetchone(\n                \"SELECT COALESCE(SUM(weight),0) FROM risk_signals WHERE object_id=%s\", \n                (oid,)\n            )[0] or 0\n            run(\"UPDATE objects SET risk_score=%s WHERE object_id=%s\", (total_risk, oid))\n            processed += 1\n            \n            if HAS_TQDM:\n                pbar.update(1)\n            elif processed % 500 == 0:  # Fallback progress without tqdm\n                print(f\"  Processed {processed:,}/{total_objects:,} ({100*processed/total_objects:.1f}%)\")\n        \n        # Commit every batch to avoid long transactions\n        conn.commit()\n    \n    if HAS_TQDM and pbar:\n        pbar.close()\n    \n    print(f\"✅ Rescoring complete! Updated {processed:,} objects.\")\n\ndef rescore_all_objects_fast():\n    \"\"\"Alternative: Single SQL query approach (faster for large datasets)\"\"\"\n    print(\"Rescoring all objects with single query...\")\n    \n    # Update all risk scores in one query\n    run(\"\"\"\n    UPDATE objects \n    SET risk_score = (\n        SELECT COALESCE(SUM(rs.weight), 0) \n        FROM risk_signals rs \n        WHERE rs.object_id = objects.object_id\n    )\n    \"\"\")\n    \n    # Get count of updated objects\n    total_count = run_fetchone(\"SELECT COUNT(*) FROM objects\")[0]\n    print(f\"✅ Fast rescoring complete! Updated {total_count:,} objects.\")\n    \n    conn.commit()\n\ndef print_counts():\n    for t in [\"objects\",\"provenance_sentences\",\"provenance_events\",\"risk_signals\"]:\n        count = run_fetchone(f'SELECT COUNT(*) FROM {t}')[0]\n        print(f\"{t}: {count:,}\")\n\ndef print_top_leads(n=10):\n    rows = run_fetchall(f\"\"\"\n    SELECT\n      o.object_id, o.source, LEFT(o.title, 80) AS title, o.risk_score,\n      (\n        SELECT GROUP_CONCAT(code ORDER BY weight DESC SEPARATOR ', ')\n        FROM (SELECT rs.code, rs.weight FROM risk_signals rs\n              WHERE rs.object_id = o.object_id ORDER BY rs.weight DESC LIMIT 3) x\n      ) AS top_signals\n    FROM objects o\n    WHERE o.risk_score > 0\n    ORDER BY o.risk_score DESC\n    LIMIT {int(n)};\n    \"\"\")\n    if not rows:\n        print(\"(no leads with risk_score > 0 yet)\")\n    else:\n        for r in rows:\n            print(f\"#{r[0]} [{r[1]}] score={r[3]:.2f} :: {r[2]} || {r[4]}\")\n\n# =============================\n# Test function to verify image URLs\n# =============================\ndef test_aic_image_urls(limit=5):\n    \"\"\"Test function to verify that image URLs are working\"\"\"\n    url = f\"https://api.artic.edu/api/v1/artworks?limit={limit}&fields=id,title,image_id\"\n    r = requests.get(url)\n    r.raise_for_status()\n    \n    data = r.json().get(\"data\", [])\n    print(f\"Testing {len(data)} AIC image URLs:\")\n    \n    for obj in data:\n        image_id = obj.get(\"image_id\")\n        if image_id:\n            image_url = f\"https://www.artic.edu/iiif/2/{image_id}/full/843,/0/default.jpg\"\n            print(f\"ID: {obj.get('id')} | Title: {obj.get('title', 'No title')[:50]}\")\n            print(f\"Image URL: {image_url}\")\n            \n            # Test if URL is accessible\n            try:\n                resp = requests.head(image_url, timeout=10)\n                status = \"✅ OK\" if resp.status_code == 200 else f\"❌ {resp.status_code}\"\n                print(f\"Status: {status}\")\n            except Exception as e:\n                print(f\"❌ Error: {e}\")\n            print(\"-\" * 80)\n        else:\n            print(f\"ID: {obj.get('id')} | No image_id available\")\n\n# =============================\n# Main\n# =============================\nif __name__ == \"__main__\":\n    print(f\"🖥️  device: {'GPU' if device=='cuda' else 'CPU'}\")\n    print(f\"📊 Progress bars: {'✅ tqdm available' if HAS_TQDM else '❌ basic progress only'}\")\n    print(\"🔧 Bootstrapping schema…\")\n    bootstrap_schema()\n\n    print(f\"▶️ AIC ingest → target {AIC_TARGET} …\")\n    aic_n = ingest_aic_paged(AIC_TARGET, AIC_PAGE_SIZE)\n    print(f\"   AIC ingested: {aic_n}\")\n\n    if CMA_TARGET and int(CMA_TARGET) > 0:\n        print(f\"▶️ CMA ingest → target {CMA_TARGET} (time cap {CMA_MAX_TIME_SEC}s) …\")\n        cma_n = ingest_cma(CMA_TARGET, CMA_PAGE_SIZE, CMA_MAX_TIME_SEC)\n        print(f\"   CMA ingested: {cma_n}\")\n    else:\n        print(\"▶️ CMA ingest skipped (CMA_TARGET=0)\")\n\n    print(\"♻️ Rescoring all objects…\")\n    rescore_all_objects()\n\n    print(\"📊 Counts:\")\n    print_counts()\n\n    print(\"\\n🏁 Top leads:\")\n    print_top_leads(10)\n\n    conn.commit()\n    print(\"\\n✅ Done. In SQL editor, run:\")\n    print(\"   SELECT * FROM flagged_leads ORDER BY risk_score DESC LIMIT 20;\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-14T21:16:08.885965Z","iopub.execute_input":"2025-09-14T21:16:08.886266Z","iopub.status.idle":"2025-09-14T21:59:40.800685Z","shell.execute_reply.started":"2025-09-14T21:16:08.886243Z","shell.execute_reply":"2025-09-14T21:59:40.799931Z"}},"outputs":[{"name":"stderr","text":"AIC Ingest:   4%|▍         | 114/3000 [10:13<4:18:41,  5.38s/obj]\n","output_type":"stream"},{"name":"stdout","text":"🖥️  device: GPU\n📊 Progress bars: ✅ tqdm available\n🔧 Bootstrapping schema…\n✅ Schema ready.\n▶️ AIC ingest → target 3000 …\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"AIC Ingest:   0%|          | 0/3000 [00:00<?, ?obj/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fa14b045ea954fb6aa9a85d8b63b91ab"}},"metadata":{}},{"name":"stdout","text":"[AIC] total ingested with provenance: 3000\n   AIC ingested: 3000\n▶️ CMA ingest skipped (CMA_TARGET=0)\n♻️ Rescoring all objects…\nRescoring 3,000 objects...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Rescoring:   0%|          | 0/3000 [00:00<?, ?obj/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a96ae3a694ca441dbba5e31ccaca194b"}},"metadata":{}},{"name":"stdout","text":"✅ Rescoring complete! Updated 3,000 objects.\n📊 Counts:\nobjects: 3,000\nprovenance_sentences: 93,046\nprovenance_events: 20,583\nrisk_signals: 21,357\n\n🏁 Top leads:\n#5764607523034295818 [AIC] score=24.00 :: Man, Woman, and Bulls || EVENT_SOLD_NAZI_WINDOW, NAZI_ERA_TRADE_WINDOW, NAZI_ERA_TRADE_WINDOW\n#8935141660703094447 [AIC] score=20.80 :: Zapata || NAZI_ERA_TRADE_WINDOW, EVENT_SOLD_NAZI_WINDOW, NAZI_ERA_TRADE_WINDOW\n#7782220156096247252 [AIC] score=20.00 :: The Assumption of the Virgin || EVENT_SEIZED, EVENT_SEIZED, EVENT_SEIZED\n#2305843009213754022 [AIC] score=18.20 :: Stack of Wheat (Thaw, Sunset) || EVENT_SOLD, EVENT_SOLD, EVENT_SOLD\n#7782220156096247226 [AIC] score=16.80 :: Sheet of Studies with the Head of the Fornarina and Hands of Madame de Senonnes || EVENT_SEIZED, EVENT_SEIZED, EVENT_SEIZED\n#8935141660703094297 [AIC] score=16.00 :: Cameo Portraying Emperor Claudius as Jupiter || NAZI_ERA_TRADE_WINDOW, NAZI_ERA_TRADE_WINDOW, NAZI_ERA_TRADE_WINDOW\n#5188146770730811399 [AIC] score=15.60 :: Water Lilies || EVENT_SOLD, EVENT_SOLD, EVENT_SOLD\n#7782220156096247148 [AIC] score=14.40 :: Bouquet of Flowers and Fruit with Blue Ribbon || PRIVATE_COLLECTION_UNKNOWN, PRIVATE_COLLECTION_UNKNOWN, PRIVATE_COLLECTION_UNKNOWN\n#6629298651489430835 [AIC] score=14.40 :: The Fountains || EVENT_SOLD, EVENT_SOLD, EVENT_SOLD\n#5476377146882583688 [AIC] score=13.80 :: On the Terrace of a Hotel in Bordighera: The Painter Jean Martin Reviews His Bil || EVENT_SEIZED, EVENT_SEIZED, EVENT_SEIZED\n\n✅ Done. In SQL editor, run:\n   SELECT * FROM flagged_leads ORDER BY risk_score DESC LIMIT 20;\n","output_type":"stream"}],"execution_count":7}]}
